

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>rlcard.agents &mdash; RLcard 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <!-- <link rel="copyright" title="DATA Lab at Texas A&M University"> -->
    <link rel="next" title="rlcard.core" href="rlcard.core.html" />
    <link rel="prev" title="rlcard.games.texasholdem" href="rlcard.games.texasholdem.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> RLcard
          

          
          </a>

          
            
            
          

          
<div role="search" id="test-search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#rlcard-a-toolkit-for-reinforcement-learning-in-card-games">RLCard: A Toolkit for Reinforcement Learning in Card Games</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#available-environments">Available Environments</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="overview.html#design-principles">Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="overview.html#rlcard-high-level-design">RLCard High-level Design</a><ul>
<li class="toctree-l3"><a class="reference internal" href="overview.html#environments">Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="overview.html#games">Games</a></li>
<li class="toctree-l3"><a class="reference internal" href="overview.html#agents">Agents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#playing-with-random-agents">Playing with Random Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#deep-q-learning-on-blackjack">Deep-Q Learning on Blackjack</a></li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#deepcfr-on-blackjack">DeepCFR on Blackjack</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="games.html">Games in RLCard</a><ul>
<li class="toctree-l2"><a class="reference internal" href="games.html#blackjack">Blackjack</a><ul>
<li class="toctree-l3"><a class="reference internal" href="games.html#state-encoding-in-this">State Encoding In this</a></li>
<li class="toctree-l3"><a class="reference internal" href="games.html#action-encoding">Action Encoding</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="games.html#limit-texas-hold-em">Limit Texas Hold’em</a></li>
<li class="toctree-l2"><a class="reference internal" href="games.html#no-limit-texas-hold-em">No-limit Texas Hold’em</a></li>
<li class="toctree-l2"><a class="reference internal" href="games.html#dou-dizhu">Dou Dizhu</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="algorithms.html">Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#deep-q-learning">Deep-Q Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#deepcfr">DeepCFR</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="development.html">Development</a><ul>
<li class="toctree-l2"><a class="reference internal" href="development.html#developping-algorithms">Developping Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="development.html#adding-new-environments">Adding New Environments</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="rlcard.envs.html">rlcard.envs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.blackjack">rlcard.envs.blackjack</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.doudizhu">rlcard.envs.doudizhu</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.env">rlcard.envs.env</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.registration">rlcard.envs.registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.simpletexasholdem">rlcard.envs.simpletexasholdem</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.envs.html#module-rlcard.envs.texasholdem">rlcard.envs.texasholdem</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.utils.html">rlcard.utils</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.utils.html#module-rlcard.utils.utils">rlcard.utils.utils</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.games.html">rlcard.games</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.blackjack.html">rlcard.games.blackjack</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.dealer">rlcard.games.blackjack.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.game">rlcard.games.blackjack.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.judger">rlcard.games.blackjack.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.player">rlcard.games.blackjack.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.blackjack.html#module-rlcard.games.blackjack.round">rlcard.games.blackjack.round</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.doudizhu.html">rlcard.games.doudizhu</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.dealer">rlcard.games.doudizhu.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.game">rlcard.games.doudizhu.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.judger">rlcard.games.doudizhu.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.player">rlcard.games.doudizhu.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.doudizhu.html#module-rlcard.games.doudizhu.round">rlcard.games.doudizhu.round</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.simpletexasholdem.html">rlcard.games.simpletexasholdem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.simpletexasholdem.html#module-rlcard.games.simpletexasholdem.dealer">rlcard.games.simpletexasholdem.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.simpletexasholdem.html#module-rlcard.games.simpletexasholdem.game">rlcard.games.simpletexasholdem.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.simpletexasholdem.html#module-rlcard.games.simpletexasholdem.judger">rlcard.games.simpletexasholdem.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.simpletexasholdem.html#module-rlcard.games.simpletexasholdem.player">rlcard.games.simpletexasholdem.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.simpletexasholdem.html#module-rlcard.games.simpletexasholdem.round">rlcard.games.simpletexasholdem.round</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rlcard.games.texasholdem.html">rlcard.games.texasholdem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.texasholdem.html#module-rlcard.games.texasholdem.dealer">rlcard.games.texasholdem.dealer</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.texasholdem.html#module-rlcard.games.texasholdem.game">rlcard.games.texasholdem.game</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.texasholdem.html#module-rlcard.games.texasholdem.judger">rlcard.games.texasholdem.judger</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.texasholdem.html#module-rlcard.games.texasholdem.player">rlcard.games.texasholdem.player</a></li>
<li class="toctree-l3"><a class="reference internal" href="rlcard.games.texasholdem.html#module-rlcard.games.texasholdem.round">rlcard.games.texasholdem.round</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">rlcard.agents</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-rlcard.agents.deep_cfr">rlcard.agents.deep_cfr</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-rlcard.agents.dqn_agent">rlcard.agents.dqn_agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-rlcard.agents.random_agent">rlcard.agents.random_agent</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rlcard.core.html">rlcard.core</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">RLcard</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>rlcard.agents</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/rlcard.agents.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="rlcard-agents">
<h1>rlcard.agents<a class="headerlink" href="#rlcard-agents" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-rlcard.agents.deep_cfr">
<span id="rlcard-agents-deep-cfr"></span><h2>rlcard.agents.deep_cfr<a class="headerlink" href="#module-rlcard.agents.deep_cfr" title="Permalink to this headline">¶</a></h2>
<p>Implements Deep CFR Algorithm.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1811.00164">https://arxiv.org/abs/1811.00164</a>.</p>
<p>The algorithm defines an <cite>advantage</cite> and <cite>strategy</cite> networks that compute
advantages used to do regret matching across information sets and to approximate
the strategy profiles of the game.    To train these networks a fixed ring buffer
(other data structures may be used) memory is used to accumulate samples to
train the networks.</p>
<dl class="class">
<dt id="rlcard.agents.deep_cfr.AdvantageMemory">
<em class="property">class </em><code class="sig-prename descclassname">rlcard.agents.deep_cfr.</code><code class="sig-name descname">AdvantageMemory</code><span class="sig-paren">(</span><em class="sig-param">info_state</em>, <em class="sig-param">iteration</em>, <em class="sig-param">advantage</em>, <em class="sig-param">action</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.AdvantageMemory" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p>
<dl class="method">
<dt id="rlcard.agents.deep_cfr.AdvantageMemory.action">
<em class="property">property </em><code class="sig-name descname">action</code><a class="headerlink" href="#rlcard.agents.deep_cfr.AdvantageMemory.action" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.AdvantageMemory.advantage">
<em class="property">property </em><code class="sig-name descname">advantage</code><a class="headerlink" href="#rlcard.agents.deep_cfr.AdvantageMemory.advantage" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.AdvantageMemory.info_state">
<em class="property">property </em><code class="sig-name descname">info_state</code><a class="headerlink" href="#rlcard.agents.deep_cfr.AdvantageMemory.info_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.AdvantageMemory.iteration">
<em class="property">property </em><code class="sig-name descname">iteration</code><a class="headerlink" href="#rlcard.agents.deep_cfr.AdvantageMemory.iteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="rlcard.agents.deep_cfr.DeepCFR">
<em class="property">class </em><code class="sig-prename descclassname">rlcard.agents.deep_cfr.</code><code class="sig-name descname">DeepCFR</code><span class="sig-paren">(</span><em class="sig-param">session</em>, <em class="sig-param">env</em>, <em class="sig-param">policy_network_layers=(32</em>, <em class="sig-param">32)</em>, <em class="sig-param">advantage_network_layers=(32</em>, <em class="sig-param">32)</em>, <em class="sig-param">num_iterations=100</em>, <em class="sig-param">num_traversals=10</em>, <em class="sig-param">learning_rate=0.0001</em>, <em class="sig-param">batch_size_advantage=None</em>, <em class="sig-param">batch_size_strategy=16</em>, <em class="sig-param">memory_capacity=10000000</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.DeepCFR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Implements a solver for the Deep CFR Algorithm.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1811.00164">https://arxiv.org/abs/1811.00164</a>.</p>
<p>Define all networks and sampling buffers/memories.    Derive losses &amp; learning
steps. Initialize the game state and algorithmic variables.</p>
<dl class="simple">
<dt>Note: batch sizes default to <cite>None</cite> implying that training over the full</dt><dd><p>dataset in memory is done by default.    To sample from the memories you
may set these values to something less than the full capacity of the
memory.</p>
</dd>
</dl>
<dl class="method">
<dt id="rlcard.agents.deep_cfr.DeepCFR.action_probabilities">
<code class="sig-name descname">action_probabilities</code><span class="sig-paren">(</span><em class="sig-param">state</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.DeepCFR.action_probabilities" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns action probabilites dict for a single batch.</p>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.DeepCFR.advantage_buffers">
<em class="property">property </em><code class="sig-name descname">advantage_buffers</code><a class="headerlink" href="#rlcard.agents.deep_cfr.DeepCFR.advantage_buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.DeepCFR.clear_advantage_buffers">
<code class="sig-name descname">clear_advantage_buffers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.DeepCFR.clear_advantage_buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.DeepCFR.reinitialize_advantage_networks">
<code class="sig-name descname">reinitialize_advantage_networks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.DeepCFR.reinitialize_advantage_networks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.DeepCFR.solve">
<code class="sig-name descname">solve</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.DeepCFR.solve" title="Permalink to this definition">¶</a></dt>
<dd><p>Solution logic for Deep CFR.</p>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.DeepCFR.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">state</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.DeepCFR.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the action for generating training data
:param state: current state</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>an action id</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>action</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.DeepCFR.strategy_buffer">
<em class="property">property </em><code class="sig-name descname">strategy_buffer</code><a class="headerlink" href="#rlcard.agents.deep_cfr.DeepCFR.strategy_buffer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.DeepCFR.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.DeepCFR.train" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="rlcard.agents.deep_cfr.FixedSizeRingBuffer">
<em class="property">class </em><code class="sig-prename descclassname">rlcard.agents.deep_cfr.</code><code class="sig-name descname">FixedSizeRingBuffer</code><span class="sig-paren">(</span><em class="sig-param">replay_buffer_capacity</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.FixedSizeRingBuffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>ReplayBuffer of fixed size with a FIFO replacement policy.</p>
<p>Stored transitions can be sampled uniformly.</p>
<p>The underlying datastructure is a ring buffer, allowing 0(1) adding and
sampling.</p>
<dl class="method">
<dt id="rlcard.agents.deep_cfr.FixedSizeRingBuffer.add">
<code class="sig-name descname">add</code><span class="sig-paren">(</span><em class="sig-param">element</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.FixedSizeRingBuffer.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds <cite>element</cite> to the buffer.</p>
<p>If the buffer is full, the oldest element will be replaced.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>element</strong> – data to be added to the buffer.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.FixedSizeRingBuffer.clear">
<code class="sig-name descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.FixedSizeRingBuffer.clear" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.FixedSizeRingBuffer.sample">
<code class="sig-name descname">sample</code><span class="sig-paren">(</span><em class="sig-param">num_samples</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.FixedSizeRingBuffer.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <cite>num_samples</cite> uniformly sampled from the buffer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>num_samples</strong> – <cite>int</cite>, number of samples to draw.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An iterable over <cite>num_samples</cite> random elements of the buffer.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.7)"><strong>ValueError</strong></a> – If there are less than <cite>num_samples</cite> elements in the buffer</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="rlcard.agents.deep_cfr.StrategyMemory">
<em class="property">class </em><code class="sig-prename descclassname">rlcard.agents.deep_cfr.</code><code class="sig-name descname">StrategyMemory</code><span class="sig-paren">(</span><em class="sig-param">info_state</em>, <em class="sig-param">iteration</em>, <em class="sig-param">strategy_action_probs</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.deep_cfr.StrategyMemory" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p>
<dl class="method">
<dt id="rlcard.agents.deep_cfr.StrategyMemory.info_state">
<em class="property">property </em><code class="sig-name descname">info_state</code><a class="headerlink" href="#rlcard.agents.deep_cfr.StrategyMemory.info_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.StrategyMemory.iteration">
<em class="property">property </em><code class="sig-name descname">iteration</code><a class="headerlink" href="#rlcard.agents.deep_cfr.StrategyMemory.iteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.deep_cfr.StrategyMemory.strategy_action_probs">
<em class="property">property </em><code class="sig-name descname">strategy_action_probs</code><a class="headerlink" href="#rlcard.agents.deep_cfr.StrategyMemory.strategy_action_probs" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-rlcard.agents.dqn_agent">
<span id="rlcard-agents-dqn-agent"></span><h2>rlcard.agents.dqn_agent<a class="headerlink" href="#module-rlcard.agents.dqn_agent" title="Permalink to this headline">¶</a></h2>
<p>DQN agent</p>
<p>The code is derived from <a class="reference external" href="https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/dqn.py">https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/dqn.py</a></p>
<p>Copyright (c) 2019 DATA Lab at Texas A&amp;M University
Copyright (c) 2016 Denny Britz</p>
<p>Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the “Software”), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:</p>
<p>The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.</p>
<p>THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.</p>
<dl class="class">
<dt id="rlcard.agents.dqn_agent.DQNAgent">
<em class="property">class </em><code class="sig-prename descclassname">rlcard.agents.dqn_agent.</code><code class="sig-name descname">DQNAgent</code><span class="sig-paren">(</span><em class="sig-param">sess, replay_memory_size=20000, replay_memory_init_size=20000, update_target_estimator_every=1000, discount_factor=0.99, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay_steps=20000, batch_size=32, action_size=2, state_shape=[2], norm_step=1000</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<dl class="method">
<dt id="rlcard.agents.dqn_agent.DQNAgent.eval_step">
<code class="sig-name descname">eval_step</code><span class="sig-paren">(</span><em class="sig-param">state</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.eval_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the action for evaluation purpose.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>numpy.array</em>) – current state</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an action id</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>action (state)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.dqn_agent.DQNAgent.feed">
<code class="sig-name descname">feed</code><span class="sig-paren">(</span><em class="sig-param">ts</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.feed" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Store data in to replay buffer and train the agent. There are several stages.</dt><dd><p>In stage 1, populate the Normalizer to calculate mean and std.
In stage 2, popolate the memory without training.
In stage 3: add transitions to the memory and train the netowrk.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>ts</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – a list of 5 elements that represent the transition</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>whether the models start training</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>is_training (boolean)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.dqn_agent.DQNAgent.feed_memory">
<code class="sig-name descname">feed_memory</code><span class="sig-paren">(</span><em class="sig-param">state</em>, <em class="sig-param">action</em>, <em class="sig-param">reward</em>, <em class="sig-param">next_state</em>, <em class="sig-param">done</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.feed_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Feed transition to memory</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state</strong> (<em>numpy.array</em>) – the current state</p></li>
<li><p><strong>action</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the performed action ID</p></li>
<li><p><strong>reward</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the reward received</p></li>
<li><p><strong>next_state</strong> (<em>numpy.array</em>) – the next state after performing the action</p></li>
<li><p><strong>done</strong> (<em>boolean</em>) – whether the episode is finished</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.dqn_agent.DQNAgent.feed_norm">
<code class="sig-name descname">feed_norm</code><span class="sig-paren">(</span><em class="sig-param">state</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.feed_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Feed state to normalizer to collect statistics</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>numpy.array</em>) – the state that will be feed into normalizer</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.dqn_agent.DQNAgent.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">state</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the action for genrating training data</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>numpy.array</em>) – current state</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an action id</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>action (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)">int</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.dqn_agent.DQNAgent.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.DQNAgent.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Train the network</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="rlcard.agents.dqn_agent.Estimator">
<em class="property">class </em><code class="sig-prename descclassname">rlcard.agents.dqn_agent.</code><code class="sig-name descname">Estimator</code><span class="sig-paren">(</span><em class="sig-param">scope='estimator', action_size=2, state_shape=[2]</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Estimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Q-Value Estimator neural network.
This network is used for both the Q-Network and the Target Network.</p>
<dl class="method">
<dt id="rlcard.agents.dqn_agent.Estimator.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">sess</em>, <em class="sig-param">s</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Estimator.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts action values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sess</strong> (<em>tf.Session</em>) – Tensorflow Session object</p></li>
<li><p><strong>s</strong> (<em>numpy.array</em>) – State input of shape [batch_size, 4, 160, 160, 3]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated
action values.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.dqn_agent.Estimator.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param">sess</em>, <em class="sig-param">s</em>, <em class="sig-param">a</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Estimator.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the estimator towards the given targets.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sess</strong> (<em>tf.Session</em>) – Tensorflow Session object</p></li>
<li><p><strong>s</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – State input of shape [batch_size, 4, 160, 160, 3]</p></li>
<li><p><strong>a</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – Chosen actions of shape [batch_size]</p></li>
<li><p><strong>y</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – Targets of shape [batch_size]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The calculated loss on the batch.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="rlcard.agents.dqn_agent.Memory">
<em class="property">class </em><code class="sig-prename descclassname">rlcard.agents.dqn_agent.</code><code class="sig-name descname">Memory</code><span class="sig-paren">(</span><em class="sig-param">memory_size</em>, <em class="sig-param">batch_size</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Memory for saving transitions</p>
<dl class="method">
<dt id="rlcard.agents.dqn_agent.Memory.sample">
<code class="sig-name descname">sample</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Memory.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Sample a minibatch from the replay memory</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a batch of states
action_batch (list): a batch of actions
reward_batch (list): a batch of rewards
next_state_batch (list): a batch of states
done_batch (list): a batch of dones</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>state_batch (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.dqn_agent.Memory.save">
<code class="sig-name descname">save</code><span class="sig-paren">(</span><em class="sig-param">state</em>, <em class="sig-param">action</em>, <em class="sig-param">reward</em>, <em class="sig-param">next_state</em>, <em class="sig-param">done</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Memory.save" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state</strong> (<em>numpy.array</em>) – the current state</p></li>
<li><p><strong>action</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the performed action ID</p></li>
<li><p><strong>reward</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the reward received</p></li>
<li><p><strong>next_state</strong> (<em>numpy.array</em>) – the next state after performing the action</p></li>
<li><p><strong>done</strong> (<em>boolean</em>) – whether the episode is finished</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="rlcard.agents.dqn_agent.Normalizer">
<em class="property">class </em><code class="sig-prename descclassname">rlcard.agents.dqn_agent.</code><code class="sig-name descname">Normalizer</code><a class="headerlink" href="#rlcard.agents.dqn_agent.Normalizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Normalizer class that tracks the running statistics for normlization</p>
<dl class="method">
<dt id="rlcard.agents.dqn_agent.Normalizer.append">
<code class="sig-name descname">append</code><span class="sig-paren">(</span><em class="sig-param">s</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Normalizer.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Append a new state and update the running statistics</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>s</strong> (<em>numpy.array</em>) – the input state</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.dqn_agent.Normalizer.normalize">
<code class="sig-name descname">normalize</code><span class="sig-paren">(</span><em class="sig-param">s</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Normalizer.normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Normalize the state with the running mean and std.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>s</strong> (<em>numpy.array</em>) – the input state</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>normalized state</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>a (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)">int</a>)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="rlcard.agents.dqn_agent.Transition">
<em class="property">class </em><code class="sig-prename descclassname">rlcard.agents.dqn_agent.</code><code class="sig-name descname">Transition</code><span class="sig-paren">(</span><em class="sig-param">state</em>, <em class="sig-param">action</em>, <em class="sig-param">reward</em>, <em class="sig-param">next_state</em>, <em class="sig-param">done</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p>
<dl class="method">
<dt id="rlcard.agents.dqn_agent.Transition.action">
<em class="property">property </em><code class="sig-name descname">action</code><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition.action" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.dqn_agent.Transition.done">
<em class="property">property </em><code class="sig-name descname">done</code><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition.done" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 4</p>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.dqn_agent.Transition.next_state">
<em class="property">property </em><code class="sig-name descname">next_state</code><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition.next_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.dqn_agent.Transition.reward">
<em class="property">property </em><code class="sig-name descname">reward</code><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition.reward" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.dqn_agent.Transition.state">
<em class="property">property </em><code class="sig-name descname">state</code><a class="headerlink" href="#rlcard.agents.dqn_agent.Transition.state" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="rlcard.agents.dqn_agent.copy_model_parameters">
<code class="sig-prename descclassname">rlcard.agents.dqn_agent.</code><code class="sig-name descname">copy_model_parameters</code><span class="sig-paren">(</span><em class="sig-param">sess</em>, <em class="sig-param">estimator1</em>, <em class="sig-param">estimator2</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.dqn_agent.copy_model_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the model parameters of one estimator to another.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sess</strong> (<em>tf.Session</em>) – Tensorflow Session object</p></li>
<li><p><strong>estimator1</strong> (<a class="reference internal" href="#rlcard.agents.dqn_agent.Estimator" title="rlcard.agents.dqn_agent.Estimator"><em>Estimator</em></a>) – Estimator to copy the paramters from</p></li>
<li><p><strong>estimator2</strong> (<a class="reference internal" href="#rlcard.agents.dqn_agent.Estimator" title="rlcard.agents.dqn_agent.Estimator"><em>Estimator</em></a>) – Estimator to copy the parameters to</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-rlcard.agents.random_agent">
<span id="rlcard-agents-random-agent"></span><h2>rlcard.agents.random_agent<a class="headerlink" href="#module-rlcard.agents.random_agent" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="rlcard.agents.random_agent.RandomAgent">
<em class="property">class </em><code class="sig-prename descclassname">rlcard.agents.random_agent.</code><code class="sig-name descname">RandomAgent</code><span class="sig-paren">(</span><em class="sig-param">action_size</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.random_agent.RandomAgent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>A random agent. Random agents is for running toy examples on the card games</p>
<dl class="method">
<dt id="rlcard.agents.random_agent.RandomAgent.eval_step">
<code class="sig-name descname">eval_step</code><span class="sig-paren">(</span><em class="sig-param">state</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.random_agent.RandomAgent.eval_step" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict the action given the curent state for evaluation.</dt><dd><p>Since the random agents are not trained. This function is equivalent to step function</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>numpy.array</em>) – an numpy array that represents the current state</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the action predicted (randomly chosen) by the random agent</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>action (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)">int</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rlcard.agents.random_agent.RandomAgent.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">state</em><span class="sig-paren">)</span><a class="headerlink" href="#rlcard.agents.random_agent.RandomAgent.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the action given the curent state in gerenerating training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>numpy.array</em>) – an numpy array that represents the current state</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the action predicted (randomly chosen) by the random agent</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>action (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)">int</a>)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="rlcard.core.html" class="btn btn-neutral float-right" title="rlcard.core" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="rlcard.games.texasholdem.html" class="btn btn-neutral float-left" title="rlcard.games.texasholdem" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright DATA Lab at Texas A&amp;M University

    </p>
  </div>
    
    
      Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>